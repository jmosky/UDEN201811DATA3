{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-b8755824e778>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-b8755824e778>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    %%bash\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Opens finder!\n",
    "%%bash\n",
    "open .\n",
    "\n",
    "set\n",
    "array\n",
    "dictionary\n",
    "list\n",
    "tuple\n",
    "\n",
    "`()` means you \"call\" the function.\n",
    "\n",
    "\n",
    ".replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-f4180564e0e6>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-f4180564e0e6>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    .reset_index()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "df2.car_make.unique()\n",
    "df.loc[(df.car_make == 'Brawn')]\n",
    "df[df['car_make'] == 'Brawn']\n",
    "df[(df.results_count >= 30000) & (df.year == 2017)]\n",
    "df.query('car_make == \"Brawn\"')\n",
    "\n",
    "len('list/df')\n",
    "\n",
    "# PANDAS ARE CUTE!\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df.info()\n",
    "df.describe()\n",
    "df.shape()\n",
    "list(df)\n",
    "df.count()\n",
    "\n",
    "df[df['Column Name'].isna() == True]\n",
    "df.isna()\n",
    "df[df['Column Name']].isnull()\n",
    "\n",
    "# See datatypes\n",
    "df.dtypes\n",
    "\n",
    "# See individual item in dataframe\n",
    "df4.iloc[0].col_name\n",
    "\n",
    "df.groupby('column_name').sum('metric').to_clipboard()\n",
    "df('column_name').value_counts()\n",
    "\n",
    "# Add column with zero based index to output.\n",
    ".reset_index()\n",
    "\n",
    "# Replace na values with zero.\n",
    "df.fillna(0,inplace=True)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "df['Column'] = pd.to_numeric(df['Column'])\n",
    "\n",
    "df['Column'].value_counts().to_dict()\n",
    "\n",
    "df = pd.read_clipboard(sep='\\t')\n",
    "\n",
    "# ERROR CHECKING\n",
    "def convert_data(d):\n",
    "    try:\n",
    "        return pd.to_numeric(d)\n",
    "    except:\n",
    "        return \"error\"\n",
    "    \n",
    "df[\"new column\"] = df[\"column of interest\"].map(convert_data)\n",
    "\n",
    "# Something like...\n",
    "df[df[\"new df\"] == \"error\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List comprehensions, map, lambda function.\n",
    "\n",
    "folks = ['jeff', ' jeff ', 'robert', 'chance', 'scouty pants']\n",
    "\n",
    "# Clean spaces using a list comprehension\n",
    "print([x.strip() for x in folks])\n",
    "\n",
    "# Clean spaces using MAP and a defined func  \n",
    "def clean_spaces(instr):\n",
    "  return instr.strip()\n",
    "\n",
    "print(list(map(clean_spaces, folks)))\n",
    "\n",
    "# Clean spaces using MAP and a lambda \n",
    "print(list(map(lambda x: x.strip(), folks)))\n",
    "\n",
    "## Comments: most people say to use list comprehensions. Imagine you had a really complex piece of logic you had to apply to each element of a list.  \n",
    "\n",
    "def terrible_function(x):\n",
    "  try:\n",
    "    do_something_else()\n",
    "  except Exception as e:\n",
    "    print(str(e))\n",
    "  finally:\n",
    "    print('done')\n",
    "\n",
    "\n",
    "result = [terrible_function(j) for j in mylist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.mean([len(x) for x in words])\n",
    "\n",
    "[len(x.split()) for x in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDEN201811DATA3/Week4_Pandas/Day2/2/Activities/02-Stu_GoodMovies\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "df = pd.read_csv('Resources/movie_scores.csv')\n",
    "\n",
    "# List all the columns the table provides.\n",
    "list(df)\n",
    "\n",
    "# And select only those related to IMDB.\n",
    "cols = [x for x in df.columns if \"IMDB\" in x]\n",
    "\n",
    "# add film to front of list.\n",
    "cols = [\"FILM\"] + cols\n",
    "imdb_df = df[cols].copy()\n",
    "print(imdb_df)\n",
    "\n",
    "# A way to filter.\n",
    "imdb_df[imdb_df[\"IMDB\"] >= 7]\n",
    "imdb_df[imdb_df[\"IMDB_user_vote_count\"] < 20000]\n",
    "\n",
    "# Another way to filter.\n",
    "imdb_df.query(\"IMDB_user_vote_count < 20000\").query(\"IMDB >= 7\")\n",
    "\n",
    "# Save results in CSV.\n",
    "imdb_df.to_csv(imdb_df.to_csv('imdb_only.csv', sep=',') #, index=False, header=True)\n",
    "\n",
    "# Copy results to CSV.\n",
    "imdb_df.to_clipboard(sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas df\n",
    "df.head(7)[[\"id\",\"Phone Number\"]]\n",
    "\n",
    "df[\"new_col\"] = df.current_col_as_function_input.map(function_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100% Bran', 'N', 'C', '70', '4', '1', '130', '10', '5', '6', '280', '25', '3', '1', '0.33', '68.402973']\n",
      "['All-Bran', 'K', 'C', '70', '4', '1', '260', '9', '7', '5', '320', '25', '3', '1', '0.33', '59.425505']\n",
      "['All-Bran with Extra Fiber', 'K', 'C', '50', '4', '0', '140', '14', '8', '0', '330', '25', '3', '1', '0.5', '93.704912']\n",
      "['Bran Flakes', 'P', 'C', '90', '3', '0', '210', '5', '13', '5', '190', '25', '3', '1', '0.67', '53.313813']\n",
      "['Fruit & Fibre Dates; Walnuts; and Oats', 'P', 'C', '120', '3', '2', '160', '5', '12', '10', '200', '25', '3', '1.25', '0.67', '40.917047']\n",
      "['Fruitful Bran', 'K', 'C', '120', '3', '0', '240', '5', '14', '12', '190', '25', '3', '1.33', '0.67', '41.015492']\n",
      "['Post Nat. Raisin Bran', 'P', 'C', '120', '3', '1', '200', '6', '11', '14', '260', '25', '3', '1.33', '0.67', '37.840594']\n",
      "['Raisin Bran', 'K', 'C', '120', '3', '1', '210', '5', '14', '12', '240', '25', '2', '1.33', '0.75', '39.259197']\n"
     ]
    }
   ],
   "source": [
    "# WARMUP\n",
    "# \"Week3\", \"Day 3\", \"Activities\", \"01-Stu_CerealCleaner\"\n",
    "import os\n",
    "import csv\n",
    "\n",
    "def filter_fiber_greather_than(fiber_amt: float):\n",
    "    csvpath = os.path.join('..',\"Week3\", \"Day 3\", \"Activities\", \"01-Stu_CerealCleaner\", \"Resources\", \"cereal.csv\")  \n",
    "    with open(csvpath, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)\n",
    "        for row in reader:        \n",
    "            fiber = float(row[7])\n",
    "            if fiber >= fiber_amt :\n",
    "                print(row)\n",
    "                \n",
    "filter_fiber_greather_than(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Jen', 'age': 32, 'hobbies': ['running', 'yoga', 'Darcy Butt'], 'waking_hours': {'Monday': 6, 'Tuesday': 5, 'Wednesday': 6, 'Sunday': 8}}\n"
     ]
    }
   ],
   "source": [
    "mememe = {\n",
    "    \"name\": \"Jen\",\n",
    "    \"age\": 32,\n",
    "    \"hobbies\": [\"running\",\"yoga\",\"Darcy Butt\"],\n",
    "    \"waking_hours\": {\n",
    "        \"Monday\": 6,\n",
    "        \"Tuesday\": 5,\n",
    "        \"Wednesday\": 6,\n",
    "        \"Sunday\": 8\n",
    "    }\n",
    "}\n",
    "\n",
    "print(mememe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm Jen and my hobbies are running and heres when I like to get up: {'Monday': 6, 'Tuesday': 5, 'Wednesday': 6, 'Sunday': 8}.\n"
     ]
    }
   ],
   "source": [
    "print(f' I\\'m {mememe[\"name\"]} and my hobbies are {mememe[\"hobbies\"][0]} and heres when I like to get up: {mememe[\"waking_hours\"]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "folks = [\"jeff\",\"Jen\",\"darcy\",\"buttimous\",\"frenchie\",\"conrad\",\"juniper\",\"joon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jeff', 'Jen', 'darcy', 'buttimous', 'frenchie', 'conrad', 'juniper', 'joon']\n"
     ]
    }
   ],
   "source": [
    "print(folks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jeff', 'Jen', 'juniper', 'joon']\n"
     ]
    }
   ],
   "source": [
    "print([x for x in folks if x.lower().startswith(\"j\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jeff', 'Jen', 'Darcy', 'Buttimous', 'Frenchie', 'Conrad', 'Juniper', 'Joon']\n"
     ]
    }
   ],
   "source": [
    "formal_folks = [x.title() for x in folks]\n",
    "print(formal_folks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dear Jane, please come to the party hard-y this Saturday!', 'Dear Julie, please come to the party hard-y this Saturday!', 'Dear Darcy, please come to the party hard-y this Saturday!', 'Dear Terry, please come to the party hard-y this Saturday!', 'Dear Pamela, please come to the party hard-y this Saturday!']\n",
      "Dear Jane, please come to the party hard-y this Saturday!\n",
      "Dear Julie, please come to the party hard-y this Saturday!\n",
      "Dear Darcy, please come to the party hard-y this Saturday!\n",
      "Dear Terry, please come to the party hard-y this Saturday!\n",
      "Dear Pamela, please come to the party hard-y this Saturday!\n",
      "<class 'list'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Exercise: Day 3 > 05-Stu List Comprehensions\n",
    "\n",
    "names = [\"Jane\",\"julie\",\"DARCY\",\"Terry\",\"PamelA\"]\n",
    "\n",
    "for people in names:\n",
    "    \n",
    "    lowercased = [people.lower() for people in names]\n",
    "    titlecased = [people.title() for people in names]\n",
    "\n",
    "invitations = [f\"Dear {name}, please come to the party hard-y this Saturday!\" for name in titlecased]\n",
    "\n",
    "print(invitations)\n",
    "\n",
    "for invitation in invitations:\n",
    "    print(invitation)\n",
    "    \n",
    "print(type(invitations))\n",
    "\n",
    "for invitation in invitations:\n",
    "    print(type(invitation))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "# @TODO: Write a function that returns the arithmetic average for a list of numbers\n",
    "# Test your function with the following:\n",
    "# print(average([1, 5, 9]))\n",
    "# print(average(range(11)))\n",
    "\n",
    "def avg_nums(list_of_nums):\n",
    "    total = sum(list_of_nums)\n",
    "    count = len(list_of_nums)\n",
    "\n",
    "    return total / count\n",
    "\n",
    "print(avg_nums([1, 5, 9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dean Ambrose 133 67 4\n",
      "Kevin Owens 61 130 2\n",
      "Tyler Breeze 17 163 2\n",
      "AJ Styles 108 68 0\n",
      "Sami Zayn 111 59 6\n",
      "Dolph Ziggler 114 56 2\n",
      "Zack Ryder 119 51 2\n",
      "Rusev 70 97 2\n",
      "Jey Uso 76 90 0\n",
      "Baron Corbin 103 59 3\n",
      "Sheamus 44 115 5\n",
      "Becky Lynch 102 54 5\n",
      "Roman Reigns 142 12 5\n",
      "Apollo Crews 125 30 0\n",
      "Jason Jordan 126 23 2\n",
      "Jimmy Uso 70 80 0\n",
      "Viktor 2 145 1\n",
      "Chad Gable 120 25 2\n",
      "The Miz 43 103 1\n",
      "Mojo Rawley 113 31 2\n",
      "Natalya 37 109 0\n",
      "Cesaro 78 62 3\n",
      "Jack Swagger 73 70 0\n",
      "Bayley 126 11 2\n",
      "Big E 107 31 1\n",
      "Fandango 24 111 0\n",
      "Kofi Kingston 95 39 1\n",
      "Kalisto 75 59 0\n",
      "Sasha Banks 101 28 5\n",
      "Bray Wyatt 24 107 2\n",
      "Sin Cara 69 62 2\n",
      "Aiden English 12 118 2\n",
      "Enzo Amore 79 48 3\n",
      "Charlotte 86 42 1\n",
      "Heath Slater 62 64 0\n",
      "Alexa Bliss 35 90 0\n",
      "Curtis Axel 11 114 0\n",
      "Simon Gotch 11 111 1\n",
      "Bo Dallas 17 104 1\n",
      "Carmella 59 62 1\n",
      "Goldust 96 26 0\n",
      "Karl Anderson 31 89 2\n",
      "Braun Strowman 65 53 3\n",
      "Titus O'Neil 47 73 1\n",
      "Kane 103 15 0\n",
      "R-Truth 96 21 1\n",
      "Seth Rollins 39 75 4\n",
      "Asuka 112 3 2\n",
      "Chris Jericho 38 77 0\n",
      "Konnor 2 110 1\n",
      "Luke Gallows 27 84 2\n",
      "Nia Jax 37 72 1\n",
      "Xavier Woods 83 26 1\n",
      "Darren Young 74 28 2\n",
      "Neville 78 26 0\n",
      "Big Cass 61 40 1\n",
      "Tye Dillinger 60 41 1\n",
      "Dash Wilder 34 64 3\n",
      "Scott Dawson 34 63 3\n",
      "D-Von Dudley 19 77 0\n",
      "Shinsuke Nakamura 90 4 2\n",
      "Samoa Joe 32 59 4\n",
      "Bubba Ray Dudley 21 72 0\n",
      "Alberto Del Rio 24 65 2\n",
      "Peyton Royce 20 67 2\n",
      "Erick Rowan 14 72 1\n",
      "Alexander Wolfe 37 49 0\n",
      "Dana Brooke 11 74 0\n",
      "Liv Morgan 42 43 0\n",
      "Naomi 41 43 0\n",
      "Angelo Dawkins 21 62 0\n",
      "Billie Kay 27 50 2\n",
      "Aliyah 26 51 1\n",
      "Patrick Clark 26 52 0\n",
      "Elias Samson 29 48 0\n",
      "The Big Show 60 16 1\n",
      "No Way Jose 56 19 1\n",
      "Alicia Fox 49 25 1\n",
      "Daria Berenato 27 47 0\n",
      "Epico 21 52 1\n",
      "Primo 19 54 1\n",
      "Riddick Moss 18 55 1\n",
      "Tino Sabbatelli 24 50 0\n",
      "Stardust 5 66 2\n",
      "Tucker Knight 28 45 0\n",
      "Finn Balor 60 11 1\n",
      "Luke Harper 20 51 1\n",
      "Sawyer Fulton 28 42 1\n",
      "Mandy Rose 13 54 1\n",
      "Andrade Almas 40 23 0\n",
      "Rhyno 50 13 0\n",
      "Austin Aries 37 23 1\n",
      "Rich Swann 41 19 1\n",
      "Randy Orton 39 21 0\n",
      "Ryback 37 17 1\n",
      "Bobby Roode 25 28 1\n",
      "Summer Rae 12 42 0\n",
      "Wesley Blake 7 45 1\n",
      "Ember Moon 36 15 0\n",
      "John Cena 29 21 0\n",
      "Oney Lorcan 32 18 0\n",
      "Curt Hawkins 4 44 0\n",
      "Nikki Bella 34 13 1\n",
      "Tommaso Ciampa 32 16 0\n",
      "Colin Cassady 38 7 2\n",
      "Johnny Gargano 32 15 0\n",
      "Steve Cutler 10 35 1\n",
      "Paige 30 15 0\n",
      "Shane Thorne 28 17 0\n",
      "Buddy Murphy 10 33 1\n",
      "Emma 14 30 0\n",
      "Manny Andrade 32 12 0\n",
      "Nick Miller 27 17 0\n",
      "Tamina 6 36 0\n",
      "Eva Marie 18 22 0\n",
      "Jinder Mahal 3 37 0\n",
      "Cedric Alexander 26 13 0\n",
      "Kenneth Crawford 16 21 0\n",
      "Christopher Girard 7 28 1\n",
      "Hugo Knox 24 11 0\n",
      "Blake 2 31 1\n",
      "Murphy 2 31 1\n",
      "Mark Henry 20 12 0\n",
      "TJ Perkins 22 10 0\n",
      "Alex Riley 11 20 0\n",
      "Charlotte Flair 12 18 1\n",
      "Sunny Dhinsa 11 19 0\n",
      "Adam Rose 6 23 0\n",
      "Damien Sandow 11 18 0\n",
      "Lince Dorado 15 13 1\n",
      "Gzim Selmani 11 17 0\n",
      "Hideo Itami 26 2 0\n",
      "Adrienne Reese 14 12 0\n",
      "Akam 23 3 0\n",
      "Rezar 23 3 0\n",
      "Dan Matha 7 17 1\n",
      "Josh Woods 9 15 0\n",
      "Lana 3 20 0\n",
      "Niko Bogojevic 7 14 0\n",
      "Adrian Jaoude 5 14 1\n",
      "Diego 1 19 0\n",
      "Otis Dozovic 7 13 0\n",
      "The Brian Kendrick 11 9 0\n",
      "Noam Dar 7 12 0\n",
      "Tony Nese 7 12 0\n",
      "Roderick Strong 13 5 0\n",
      "Eric Young 8 9 0\n",
      "King Barrett 3 14 0\n",
      "Brie Bella 7 9 0\n",
      "Drew Gulak 3 13 0\n",
      "Fernando 1 15 0\n",
      "Kishan Raftar 6 10 0\n",
      "Levis Valenzuela Jr. 15 1 0\n",
      "Ariya Daivari 3 12 0\n",
      "Jack Gallagher 12 3 0\n",
      "Nicola Glencross 3 12 0\n",
      "Noah Kekoa 5 10 0\n",
      "Tian Bing 6 8 0\n",
      "Gran Metalik 8 5 0\n",
      "Nikki Cross 8 5 0\n",
      "Noah Potjes 3 10 0\n",
      "Brock Lesnar 8 3 0\n",
      "Cezar Bononi 1 9 1\n",
      "James Ellsworth 8 3 0\n",
      "Macey Estrella 1 10 0\n"
     ]
    }
   ],
   "source": [
    "# wrestler won, lost, and drew \n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "csvpath = os.path.join('..',\"Week3\", \"Day 3\", \"Activities\", \"08-Par_WrestlingWithFunctions\", \"Resources\", \"WWE-Data-2016.csv\")  \n",
    "\n",
    "def getPercentages(nameToCheck):\n",
    "    with open(csvpath, 'r') as f:\n",
    "        # Do I need these?\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            name, wins, loss, draw = row\n",
    "            wins_int = int(wins)\n",
    "            loss_int = int(loss)\n",
    "            print(name, wins, loss, draw)\n",
    "            if nameToCheck == row[0]:\n",
    "                pass\n",
    "#                 print(type(row[2]))\n",
    "#                 nums = [int(x) for x in row[1:4]]\n",
    "#                 break\n",
    "#                 winPercent = row[1] / sum(int(row[1:4]))\n",
    "#                 lossPercent = row[2] / sum(row[1:4])\n",
    "#                 drawPercent = row[3] / sum(row[1:4])\n",
    "            \n",
    "getPercentages(\"Dean Ambrose\")\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n"
     ]
    }
   ],
   "source": [
    "testlist = [\"Dean Ambrose\",133,67,4]\n",
    "matches = sum(testlist[1:4])\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = ['1', '2', '3']\n",
    "sum(list(map(int, nums)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[int(x) for x in nums]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Jenita\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jenita\n"
     ]
    }
   ],
   "source": [
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jenniferwilson/Desktop/Repositories/UDEN201811DATA3/Notes'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-73b9668710f0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-73b9668710f0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    cd ..\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "cd ..\n",
    "cd UDEN201811DATA3/Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-c8068aad21df>, line 207)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-c8068aad21df>\"\u001b[0;36m, line \u001b[0;32m207\u001b[0m\n\u001b[0;31m    firstname, lastname, ss = row[0], row[1], row[]\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# function vs. method\n",
    "# variable assignments and when/where\n",
    "# computationally expensive\n",
    "# \"the try except block indentations in expressions\"\n",
    "\n",
    "data = [1, 2, 55, 78, 35]\n",
    "outcomes = []\n",
    "\n",
    "for index, val in enumerate(data):\n",
    "  try:\n",
    "    current_val = val\n",
    "    next_val = data[index + 1]\n",
    "    print(current_val < next_val)\n",
    "    print(current_val, next_val)\n",
    "    is_sorted = current_val < next_val\n",
    "    outcomes.append(is_sorted)\n",
    "  except IndexError:\n",
    "    pass\n",
    "\n",
    "print(is_sorted)\n",
    "print(outcomes)\n",
    "\n",
    "print(data)\n",
    "\n",
    "print(min(data))\n",
    "print(max(data))\n",
    "print(data[::-1])\n",
    "print(data[-1])\n",
    "print(data[:2])\n",
    "\n",
    "for item in data:\n",
    "  print(item)\n",
    "\n",
    "  # SHIFT COMMAND V \n",
    " \n",
    "# name = \"Jennifer Sunshine Wilson\"\n",
    "age = 30.76\n",
    "profession = \"Human\"\n",
    "dogs = 1\n",
    "\n",
    "# new way\n",
    "greeting = f\"Hi my name is {name} and I am {age - 10} years old\"\n",
    "\n",
    "# okay way\n",
    "# print(\"Hi my name is \", name, \"and I am \", age, \"years old.\")\n",
    "\n",
    "# old way (pre-3.6)\n",
    "# print(\"Hi my name is {} and I am {} years old\".format(name, age))\n",
    "\n",
    "# print(f\"Hi my name is {name} and I am {age - 10} years old.\")\n",
    "\n",
    "# print(greeting)\n",
    "# print(type(age))\n",
    "# print(type(greeting))\n",
    "\n",
    "# print(greeting[:10])\n",
    "# print(len(greeting))\n",
    "\n",
    "# words = greeting.split()\n",
    "# print(words)\n",
    "\n",
    "# name.startswith(\"J\")\n",
    "\n",
    "words = [\"jen\",\"dog\",\"moose\"]\n",
    "for w in words:\n",
    "    if w.startswith(\"j\"):\n",
    "        # This must be indented.\n",
    "        print(w, \"does in fact start with j\")\n",
    "\n",
    "\n",
    "# assign a letter grade\n",
    "\n",
    "score = 78\n",
    "\n",
    "def get_grade(some_score):\n",
    "    if some_score >=90:\n",
    "        letter_grade = \"A\"\n",
    "    elif some_score >= 80:\n",
    "        letter_grade = \"B\"\n",
    "    elif some_score >= 70:\n",
    "        letter_grade = \"C\"\n",
    "    elif some_score >= 60:\n",
    "        letter_grade = \"D\"\n",
    "    else:\n",
    "        letter_grade = \"F\"\n",
    "    \n",
    "    return letter_grade\n",
    "\n",
    "print(get_grade(score))\n",
    "\n",
    "\n",
    "\n",
    "# Incorporate the random library\n",
    "import random\n",
    "\n",
    "# Print Title\n",
    "print(\"Let's Play Rock Paper Scissors!\")\n",
    "\n",
    "# Specify the three options\n",
    "options = [\"r\", \"p\", \"s\"]\n",
    "\n",
    "# Computer Selection\n",
    "computer_choice = random.choice(options)\n",
    "\n",
    "# User Selection\n",
    "user_choice = input(\"Make your Choice: (r)ock, (p)aper, (s)cissors? \")\n",
    "\n",
    "# Run Conditionals\n",
    "if computer_choice == user_choice:\n",
    "    print(\"Tie!\")\n",
    "elif computer_choice == 'r' and user_choice == 'p':\n",
    "    print(\"Computer wins!\")\n",
    "elif computer_choice == 'r' and user_choice == 's':\n",
    "    print(\"Computer wins!\")\n",
    "elif computer_choice == 'p' and user_choice == 'r':\n",
    "        print(\"User wins!\")\n",
    "elif computer_choice == 'p' and user_choice == 's':\n",
    "        print(\"User wins!\")\n",
    "elif computer_choice == 's' and user_choice == 'r':\n",
    "    print(\"User wins!\")\n",
    "elif computer_choice == 's' and user_choice == 'p':\n",
    "    print(\"Computer wins!\")\n",
    "else:\n",
    "    print(\"Error\")\n",
    "\n",
    "### Warmup Activity #1 - lists and iteration, plus string operations\n",
    "\n",
    "people = [\"John\", \"Jack\", \"Mr Ed\", \"Ronald\", \"Becky\", \n",
    "    \"Dianne\", \"Germaine\", \"Horas\", \"Ed\", \"Edd\", \"Wilbur\",\n",
    "    \"Constantine\", \"Scouty Pants\"\n",
    "]\n",
    "\n",
    "# 1) Find names (create a new list) which either start or end with \"ed\"\n",
    "# 2) Sort the names alphabetically using the sorted function, which takes a list\n",
    "# 3) Sort the names based on the length of the name, google \"python sort list based on length of string\"\n",
    "\n",
    "ed_names = []\n",
    "names_count = 0\n",
    "for name in people:\n",
    "    names_count += 1\n",
    "    if name.lower().endswith(\"ed\") or name.lower().startswith(\"ed\"):\n",
    "        ed_names.append(name)\n",
    "\n",
    "print(sorted(people))\n",
    "\n",
    "# Alphabetical list.\n",
    "print(sorted(ed_names))\n",
    "\n",
    "# List by character length.\n",
    "print(sorted(ed_names, key=len)) # does not modify original\n",
    "people.sort(key=len) # modifies original\n",
    "print(sorted(people, key=lambda x: len(x)))\n",
    "\n",
    "# Show number of names in original list.\n",
    "print(names_count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# First we'll import the os module\n",
    "# This will allow us to create file paths across operating systems\n",
    "import os\n",
    "\n",
    "# Module for reading CSV files\n",
    "import csv\n",
    "\n",
    "csvpath = os.path.join('..', 'Resources', 'accounting.csv')\n",
    "\n",
    "# # Method 1: Plain Reading of CSV files\n",
    "# with open(csvpath, 'r') as file_handler:\n",
    "#     lines = file_handler.read()\n",
    "#     print(lines)\n",
    "#     print(type(lines))\n",
    "\n",
    "\n",
    "# Method 2: Improved Reading using CSV module\n",
    "\n",
    "# This is a context manager. Once excited, the connection is closed.\n",
    "with open(csvpath, newline='') as csvfile:\n",
    "\n",
    "    # CSV reader specifies delimiter and variable that holds contents\n",
    "    csvreader = csv.reader(csvfile, delimiter=',')\n",
    "\n",
    "    print(csvreader)\n",
    "\n",
    "    # Read the header row first (skip this step if there is now header)\n",
    "    csv_header = next(csvreader)\n",
    "    print(f\"CSV Header: {csv_header}\")\n",
    "\n",
    "    # Read each row of data after the header\n",
    "    for row in csvreader:\n",
    "        print(row)\n",
    "\n",
    "        first, last, ss = row\n",
    "        print(first, last, ss)\n",
    "        \n",
    "        firstname, lastname, ss = row[0], row[1], row[]\n",
    "        # how to print only the first column\n",
    "        first_name = row[0]\n",
    "\n",
    "import os\n",
    "import csv\n",
    "csvpath = os.path.join('..', 'Resources', 'netflix_ratings.csv')\n",
    "\n",
    "# selection = input(\"What shall we find?\")\n",
    "selection = \"Gossip Girl\"\n",
    "\n",
    "with open(csvpath, newline='') as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',')\n",
    "\n",
    "    # print(csvreader)\n",
    "\n",
    "    for row in csvreader:\n",
    "        movie, rating, rating_score = row[0], row[1], row[5]\n",
    "        if movie == selection:\n",
    "            print(f\"{movie} is rated {rating} with a rating of {rating_score}.\")\n",
    "            break\n",
    "\n",
    "    for row in csvreader:\n",
    "        if row[0] == selection:\n",
    "            print( \" | \".join(row))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Resources/netflix_ratings.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2045e8bf4287>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mfindMovie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gossip Girl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-2045e8bf4287>\u001b[0m in \u001b[0;36mfindMovie\u001b[0;34m(movie)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfindMovie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Resources/netflix_ratings.csv'"
     ]
    }
   ],
   "source": [
    "# Modules\n",
    "import os\n",
    "import csv\n",
    "\n",
    "csvpath = os.path.join('Resources', 'netflix_ratings.csv')\n",
    "\n",
    "def findMovie(movie):\n",
    "    with open(csvpath, newline=\"\") as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=\",\")\n",
    "        next(reader)\n",
    "        for line in reader:\n",
    "            if line[0] == movie:\n",
    "                print(line)\n",
    "\n",
    "findMovie(\"Gossip Girl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'Resources/dataSet.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3bd76077b46f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Use Pandas to read data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'Resources/dataSet.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# Save path to data set in a variable\n",
    "data_file = \"Resources/dataSet.csv\"\n",
    "\n",
    "# Use Pandas to read data\n",
    "df = pd.read_csv(data_file)\n",
    "df.head()\n",
    "\n",
    "# Explore the data.\n",
    "df.groupby(\"Gender\").mean()[\"Amount\"]\n",
    "df.Gender.unique()\n",
    "df.Gender.value_counts()\n",
    "\n",
    "df.head()\n",
    "\n",
    "df['Adjusted Amount'] = df.Amount / 1000\n",
    "# This notation is preferred while learning.\n",
    "df['Adjusted Amount_2'] = df['Amount'] + 50\n",
    "\n",
    "\n",
    "# Females with amount > 10,000\n",
    "# This returns a boolean mask (row: true/false)\n",
    "df[(df[\"Gender\"] == 'F') & (df['Amount'] > 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.reac_csv('conversion_paths.csv')\n",
    "\n",
    "df['path_list'] = df['MCF Channel Grouping Path'].str.split(\">\")\n",
    "df['path list'] = df['path_list'].map(lambda x: [j.strip() for j in x])\n",
    "df['first_path'] = df['path_list'].map(lambda x: x[0])\n",
    "df['Conversion Value'] = df['Conversion Value'].map(lambda x: float(x.replace(\"$\",\"\").replace(\",\", \" \")))\n",
    "df.head\n",
    "df.groupby('first_path').sum()['Conversion Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique items \n",
    "\n",
    "import counter\n",
    "\n",
    "# Generate list of counts by unique item in dataset.\n",
    "new_list_from_counter = Counter(input_data_to_count)\n",
    "# Convert to dictionary\n",
    "new_dict = dict(Counter(new_list_from_counter))\n",
    "\n",
    "# Outputs percentage frequency by unique item.\n",
    "# For every key in the new_dict, give me the value of that item in the dict and divide it by the number\n",
    "# of rows in original list.\n",
    "# This is a dectionary comprehension.\n",
    "{k: v / len(new_list_from_counter) for k,v in new_dict.items()}\n",
    "\n",
    "for unique_item, freq_pct in new_dict.items():\n",
    "    print(f\"{unique item} and {freq_pct * 100}%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDEN201811DATA3/Week4_Pandas/Day2/2/Activities/07-Par_Pokemon/Unsolved\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"Resources/Pokemon.csv\")\n",
    "avg_stats_df['Power'] = avg_stats_df[[\"HP\",\"Attack\",\"Sp. Atk\",\"Sp. Def\",\"Speed\"]].sum()\n",
    "subset_df = df[[\"Type 1\", \"HP\", \"Attack\", \"Sp. Atk\", \"Sp. Def\", \"Speed\"]]\n",
    "avg_stats_df = subset_df.groupby('Type 1').mean()\n",
    "avg_stats_df.head()\n",
    "\n",
    "avg_stats_df['Power'] = avg_stats_df[[\"HP\",\"Attack\",\"Sp. Atk\",\"Sp. Def\",\"Speed\"]].sum(axis=1)\n",
    "avg_stats_df['Power'] = avg_stats_df.sum(axis=1)\n",
    "avg_stats_df.sort_values(by=\"Power\",ascending=False).to_csv('poke_out.csv', header=True, index=False)\n",
    "\n",
    "print(avg_stats_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e67f8f16a0b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Big summary here!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------------------- \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "with open(output.txt,'w'):\n",
    "    f.write(\"Big summary here!\")\n",
    "    f.write(\"------------------- \\n\")\n",
    "    for x in rows:\n",
    "        f.write(x[0] + \"\\n\")\n",
    "        \n",
    "\n",
    "import numpy as np\n",
    "np.percentile(nums, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BINNING\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "csv_path = \"Resources/ted_talks.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "df.head()\n",
    "\n",
    "min_views = df['views'].min()\n",
    "max_views = df['views'].max()\n",
    "\n",
    "print(min_views)\n",
    "print(max_views)\n",
    "\n",
    "# Percentiles.\n",
    "for val in np.arange(0,100,10):\n",
    "    print(f'{val}th % ile, {round(np.percentile(df.views, val),0)}')\n",
    "    \n",
    "bins = [5,50,100,150,200,300,500,5000]\n",
    "bins = [x * 10000 for x in bins]\n",
    "labels = [\"<50K\",\"50-100K\",\"100-150K\",\"150-200K\",\">300K\",\"hope\",\"max bin\"]\n",
    "\n",
    "pd.cut(df.views, bins=bins, labels=labels)\n",
    "#dataframe without formatting is a series.\n",
    "\n",
    "df['bins'] = pd.cut(df.views, bins=bins, labels=labels)\n",
    "df.head(10)\n",
    "\n",
    "df.groupby(\"bins\").count()\n",
    "df.groupby(\"bins\").count()['title']\n",
    "\n",
    "# Event bins.\n",
    "even_bins = pd.cut(df.views,10).value_counts()\n",
    "print(even_bins)\n",
    "\n",
    "df.groupby['bins'].mean()\n",
    "\n",
    "# Apply multiple types of aggregations at once.\n",
    "df.groupby['bins'].agg(['count','mean'])\n",
    "\n",
    "df.groupby[['events','bins']].agg(['count','mean'])\n",
    "# Move things to column names.\n",
    "df.groupby[['events','bins']].agg(['count','mean']).unstack['bins']['title']\n",
    "\n",
    ".reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Map to format all the columns. Formatting columns.\n",
    "file_df[\"avg_cost\"] = file_df[\"avg_cost\"].map(\"${:.2f}\".format)\n",
    "file_df[\"population\"] = file_df[\"population\"].map(\"{:,}\".format)\n",
    "file_df[\"other\"] = file_df[\"other\"].map(\"{:.2f}\".format)\n",
    "file_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.assign(avg_words=df['feedback'].str.split().str.len()) \\\n",
    "    ...:   .groupby('id') \\\n",
    "    ...:   .agg({'id': 'count','feedback': 'count', 'avg_words': 'mean'}) \\\n",
    "    ...:   .rename(columns={'id':'count', 'feedback':'complete'}) \\\n",
    "    ...:   .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.log() --> level, np.exp(logvalue) --> original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATPLOTLIB\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "months = np.arange(1,13,1) # creates months in a year\n",
    "# alternate: #[x + 10 for x in range(1,13,1)]\n",
    "farenheit = [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]\n",
    "\n",
    "celsius = [(x - 32 * (5/9)) for x in farenheit] #(32°F − 32) × 5/9 = 0°\n",
    "\n",
    "plt.plot(months,celsius,marker ='s', color='red', label=\"Celsius\")\n",
    "plt.plot(months,farenheit, marker ='o', color='blue', label=\"Farenheit\")\n",
    "# Set our legend to where the chart thinks is best\n",
    "plt.legend(handles=[fahrenheit, celcius], loc=\"best\")\n",
    "#plt.legend(loc=1)\n",
    "plt.xlabel(\"Months\")\n",
    "plt.ylabel(\"Degrees\")\n",
    "\n",
    "# plt.xlim(0,10)\n",
    "# plt.ylim(-1,1)\n",
    "\n",
    "# plt.savefig(\"../Images/lineConfig.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMPY\n",
    "\n",
    "# Set x axis to numerical value for month\n",
    "months = np.arange(1,13,1)\n",
    "months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create an array that contains the number of users each language has\n",
    "users = [13000, 26000, 52000, 30000, 9000]\n",
    "x_axis = np.arange(len(users))\n",
    "\n",
    "# Tell matplotlib that we will be making a bar chart\n",
    "# Users is our y axis and x_axis is, of course, our x axis\n",
    "# We apply align=\"edge\" to ensure our bars line up with our tick marks\n",
    "plt.bar(x_axis, users, color='r', alpha=0.5, align=\"center\")\n",
    "\n",
    "# Tell matplotlib where we would like to place each of our x axis headers\n",
    "tick_locations = [value for value in x_axis]\n",
    "plt.xticks(tick_locations, [\"Java\", \"C++\", \"Python\", \"Ruby\", \"Clojure\"])\n",
    "\n",
    "# Sets the x limits of the current chart\n",
    "plt.xlim(-0.75, len(x_axis)-0.25)\n",
    "\n",
    "# Sets the y limits of the current chart\n",
    "plt.ylim(0, max(users)+5000)\n",
    "\n",
    "# Give our chart some labels and a tile\n",
    "plt.title(\"Popularity of Programming Languages\")\n",
    "plt.xlabel(\"Programming Language\")\n",
    "plt.ylabel(\"Number of People Using Programming Languages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame Bar Chart\n",
    "df.sort_values(\"Inches\",ascending=False).plot(x=\"State\",y=\"Inches\",kind=\"bar\",figsize=(12,5),title=\"Avg. Annual Rain Per State\")\n",
    "\n",
    "df.hist(\"Inches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Resources/got.csv\")\n",
    "\n",
    "attacker = df.groupby('attacker_king')['attacker_king'].count()\n",
    "defender = df.groupby('defender_king')['defender_king'].count()\n",
    "\n",
    "attacker_2 = df['attacker_king'].value_counts()\n",
    "\n",
    "# Get total battle data\n",
    "total_battle = pd.concat([attacker, defender], axis=1,sort=True).reset_index()\n",
    "total_battle.fillna(0, inplace=True)\n",
    "total_battle['total_battles'] = total_battle.attacker_king + total_battle.defender_king\n",
    "total_battle.head()\n",
    "total_battle['total_battles'].plot(kind='bar')\n",
    "\n",
    "# Do it manually using Python dictionary!\n",
    "# a = attacker.to_dict()\n",
    "# d = defender.to_dict()\n",
    "# {k:v + a.get(k, 0) for k,v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_df = df['gender'].value_counts().reset_index()\n",
    "gender_df.columns=[\"gender\",\"gender_count\"]\n",
    "gender_df[gender_df[\"gender\"] != 'stoptime'] #alternative: gender_df[gender_df.gender != 'stoptime']\n",
    "\n",
    "out = df.groupby('gender').count()['bikeid'].to_frame().drop('stoptime') #.plot(kind='bar')\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APIs, dataframe, merging dataframes, concatenating dataframes\n",
    "\n",
    "import pandas as pd \n",
    "import requests\n",
    "data = requests.get(\"https://api.iextrading.com/1.0/stock/aapl/chart/1y\")\n",
    "data = data.json() #request will parse response into JSON or error if cannot parse.\n",
    "df = pd.DataFrame.from_records(data) # record is analagous to dictionary\n",
    "\n",
    "def get_stock_price_df(ticker):\n",
    "    # f\"https://api.iextrading.com/1.0/stock/{}/chart/1y\".format(ticker)\n",
    "    data = requests.get(f\"https://api.iextrading.com/1.0/stock/{ticker}/chart/1y\")\n",
    "    response = data.json() #request will parse response into JSON or error if cannot parse.\n",
    "    df = pd.DataFrame.from_records(response) # record is analagous to dictionary\n",
    "    df['ticker'] = ticker\n",
    "    return df\n",
    "\n",
    "stocks = ['nvda','c','cat','vnq','spy']\n",
    "frames = [get_stock_price_df(ticker) for ticker in stocks]\n",
    "merged = pd.concat(frames, axis=0)\n",
    "merged.ticker.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1a6d271ed013>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Pretty print JSON for all launchpads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Pretty print JSON for all launchpads\n",
    "url = \"https://api.spacexdata.com/v2/launchpads\"\n",
    "response = requests.get(url).json()\n",
    "# print(json.dumps(response, indent=4, sort_keys=True))\n",
    "\n",
    "total_sum = 0\n",
    "good_sum = 0 \n",
    "\n",
    "for pad in response:\n",
    "#     print(pad.get('full_name'))\n",
    "#     print(pad.get('successful_launches'), \"GOOD\")\n",
    "#     print(pad.get('attempted_launches'), \"TRIES\")\n",
    "    name = pad.get('full_name')\n",
    "    attempted = pad.get('attempted_launches')\n",
    "    good = pad.get('successful_launches')\n",
    "    total_sum += attempted\n",
    "    good_sum += good\n",
    "    \n",
    "print(total_sum)\n",
    "\n",
    "response[0].keys()\n",
    "\n",
    "for row in response:\n",
    "    print(row.keys(), row.get('full_name'))  # could use bracket notation, but get will return NONE instead of throwing a key error where the value isn't present.\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# New Dependency! Use this to pretty print the JSON\n",
    "# https://docs.python.org/3/library/pprint.html\n",
    "from pprint import pprint\n",
    "\n",
    "# Note that the ?t= is a query param for the t-itle of the\n",
    "# movie we want to search for.\n",
    "url = \"http://www.omdbapi.com/?t=\"\n",
    "api_key = \"&apikey=trilogy\"\n",
    "\n",
    "# Performing a GET request similar to the one we executed\n",
    "# earlier\n",
    "response = requests.get(url + \"Aliens\" + api_key)\n",
    "print(response.url)\n",
    "\n",
    "data = response.json()\n",
    "pprint(data)\n",
    "\n",
    "# Print a few keys from the response JSON.\n",
    "print(f\"Movie was directed by {data['Director']}.\")\n",
    "print(f\"Movie was released in {data['Country']}.\")\n",
    "movies = ['rambo','when harry met sally','cars','top gun', 'fatal attraction', 'kung fu panda']\n",
    "\n",
    "data = {}\n",
    "\n",
    "for m in movies:\n",
    "    endpoint= f\"http://www.omdbapi.com?t={m}&apikey=trilogy\"\n",
    "    print(endpoint)\n",
    "    response = requests.get(endpoint).json()\n",
    "    ratings = response.get('Ratings')\n",
    "    data[m] = ratings\n",
    "    \n",
    "data = {}\n",
    "\n",
    "for m in movies:\n",
    "    endpoint = f\"http://www.omdbapi.com/?t={m}&apikey=trilogy\"\n",
    "    resp = requests.get(endpoint).json()\n",
    "    if eval(resp[\"Response\"]):\n",
    "        ratings = resp.get('Ratings', \"no rating or bad response\")\n",
    "        data[m] = ratings\n",
    "    else:\n",
    "        print(endpoint, 'is jacked and i shall not set a key in the dict')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "# http://api.open-notify.org/iss-now.json\n",
    "# http://open-notify.org/Open-Notify-API/ISS-Location-Now/\n",
    "\n",
    "# The ISS is traveling something like 5 miles per second. \n",
    "# Let's write a loop that grabs its position every 3 seconds, for 10 iterations\n",
    "# Once we have the lat longs, let's compute the total distance\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import math\n",
    "import requests\n",
    "\n",
    "import geopy.distance # pip install geopy, or conda install geopy\n",
    "\n",
    "coords_1 = (52.2296756, 21.0122287)\n",
    "coords_2 = (52.406374, 16.9251681)\n",
    "\n",
    "print (geopy.distance.distance(coords_1, coords_2).km)\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "# Write code that pings the ISS API every 3 seconds (google \"time.sleep\") \n",
    "# and adds the result to a list\n",
    "# Let it run 20 iterations\n",
    "\n",
    "# PLEASE GET THIS How much total distance did the space station travel from the first to the last API call?\n",
    "# BONUS: what's the avg distance between API calls that the ISS Travels\n",
    "# BIG BONUS: at any given point, how close is it to Denver\n",
    "# GENIUS BONUS: how do you know if it lapped the planet?\n",
    "\n",
    "\n",
    "# Stater code below - some errors, but will get you going\n",
    "pings = [] #an idea\n",
    "\n",
    "for x in range(20):\n",
    "    resp = requests.get('http://api.open-notify.org/iss-now.json').json()\n",
    "    pings.append(resp)\n",
    "    time.sleep(3)\n",
    "    print(x, \"running\")\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "pings[0].get('iss_position')\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "pings[-1]\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "# if you finish that, treat yourself to 5 random jokes and write them to a text file called jokes.txt\n",
    "# https://api.chucknorris.io/jokes/random\n",
    "coords_1 = (pings[0].get('iss_position').get('latitude'), \n",
    "            pings[0].get('iss_position').get('longitude'))\n",
    "            \n",
    "coords_2 = (pings[-1].get('iss_position').get('latitude'), \n",
    "            pings[-1].get('iss_position').get('longitude'))\n",
    "\n",
    "geopy.distance.distance(coords_1, coords_2).km\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "# ping1 to ping2 = d?\n",
    "# ping2 to ping3 = d?\n",
    "\n",
    "def create_tuple(df_object):\n",
    "    return (df_object.lat, df_object.lng)\n",
    "\n",
    "def compute_distance_between(df_object):\n",
    "    try:\n",
    "        return geopy.distance.distance(df_object.lat_lng, df_object.next_lat_lng).km\n",
    "    except ValueError:\n",
    "        # if other errors happen, let em raise and break my code\n",
    "        return 0\n",
    "\n",
    "df = pd.DataFrame.from_records(pings) # likes to eat lists of dictionaries\n",
    "df['lat'] = df.iss_position.map(lambda x: float(x.get('latitude')))\n",
    "df['lng'] = df.iss_position.map(lambda x: float(x.get('longitude')))\n",
    "df['lat_lng'] = df.apply(create_tuple, axis=1)\n",
    "df['next_lat_lng'] = df.lat_lng.shift() # very handy for offsets\n",
    "df['distance'] = df.apply(compute_distance_between, axis=1)\n",
    "\n",
    "# compute dist between \n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "data = [1,2,3,4]\n",
    "list(zip(data, data[1:])) # shift without pandas\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "df.distance.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://api.open-notify.org/iss-now.json\n",
    "# http://open-notify.org/Open-Notify-API/ISS-Location-Now/\n",
    "\n",
    "# The ISS is traveling something like 5 miles per second. \n",
    "# Let's write a loop that grabs its position every 3 seconds, for 10 iterations\n",
    "# Once we have the lat longs, let's compute the total distance\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import math\n",
    "import requests\n",
    "import numpy as np\n",
    "import geopy.distance # pip install geopy, or conda install geopy\n",
    "\n",
    "# Write code that pings the ISS API every 3 seconds (google \"time.sleep\") \n",
    "# and adds the result to a list\n",
    "# Let it run 20 iterations\n",
    "\n",
    "# Stater code below - some errors, but will get you going\n",
    "pings = [] #an idea\n",
    "distance = []\n",
    "denver = [39.7392, -104.9903]\n",
    "distance_denver = []\n",
    "\n",
    "for x in range(20):\n",
    "    endpoint = \"http://api.open-notify.org/iss-now.json\"\n",
    "    response = requests.get(endpoint).json()\n",
    "    coords = (float(response.get('iss_position').get('latitude')), float(response.get('iss_position').get('longitude')))\n",
    "    pings.append(coords)\n",
    "    time.sleep(3) # block everything for 3 seconds\n",
    "    \n",
    "# Distance traveled and distance to Denver.\n",
    "for i in range(len(pings)-1):\n",
    "    distance.append(geopy.distance.distance(pings[i],pings[i+1]).km)\n",
    "    distance_denver.append(geopy.distance.distance(pings[i],denver).km)\n",
    "print(sum(distance))\n",
    "print(distance_denver)\n",
    "\n",
    "# avg distance between API calls that the ISS Travelsaverage = np.average(distance)\n",
    "average = np.average(distance)\n",
    "print(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',-1)\n",
    "jokes = []\n",
    "for x in range(3):\n",
    "    resp = requests.get(\"https://api.chucknorris.io/jokes/random\").json()\n",
    "    jokes.append(resp)\n",
    "\n",
    "df = pd.DataFrame.from_records(jokes)\n",
    "df = df[['value','category']]\n",
    "df[\"length\"] = df['value'].str.len()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# list of tv show titles to query\n",
    "tv_shows = [\"Altered Carbon\", \"Grey's Anatomy\", \"This is Us\", \"The Flash\",\n",
    "            \"Vikings\", \"Shameless\", \"Arrow\", \"Peaky Blinders\", \"Dirk Gently\"]\n",
    "\n",
    "# tv maze show search base url\n",
    "base_url = \"http://api.tvmaze.com/search/shows?q=\"\n",
    "\n",
    "# set up lists to hold response data for name and rating\n",
    "titles = []\n",
    "ratings = []\n",
    "networks = []\n",
    "\n",
    "# loop through tv show titles, make requests and parse\n",
    "for show in tv_shows:\n",
    "    target_url = base_url + show\n",
    "    response = requests.get(target_url).json()\n",
    "    titles.append(response[0]['show']['name'])\n",
    "    ratings.append(response[0]['show']['rating']['average'])\n",
    "    \n",
    "# create dataframe\n",
    "shows_df = pd.DataFrame({\n",
    "    \"title\": titles,\n",
    "    \"rating\": ratings\n",
    "})\n",
    "\n",
    "shows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a list\n",
    "\n",
    "pairs = [\n",
    "    (40.7128, -74.0060),\n",
    "    (39.7392, -104.9903),\n",
    "    (37.2982, -113.0263),\n",
    "    (32.2226, -110.9747),\n",
    "    (39.0997,  -94.5786)\n",
    "]\n",
    "\n",
    "all_pairs =[]\n",
    "distance_between = []\n",
    "\n",
    "for row in combinations(pairs, 2):\n",
    "    all_pairs.append(row)\n",
    "    \n",
    "for row in all_pairs:    \n",
    "    c1,c2 = row\n",
    "    distance_between.append(geopy.distance.distance(c1, c2).km)\n",
    "    # results[(c1,c2)] = geopy.distance.distance(c1, c2).km\n",
    "\n",
    "# Find the 2 places that are closest to one another\n",
    "# Find the 2 places that are furthest away    \n",
    "print(\"Min: \", all_pairs[distance_between.index(min(distance_between))])\n",
    "print(\"Max: \", all_pairs[distance_between.index(max(distance_between))])\n",
    "\n",
    "# Using a dictionary. \n",
    "\n",
    "pairs = [\n",
    "    (40.7128, -74.0060),\n",
    "    (39.7392, -104.9903),\n",
    "    (37.2982, -113.0263),\n",
    "    (32.2226, -110.9747),\n",
    "    (39.0997,  -94.5786)\n",
    "]\n",
    "\n",
    "all_pairs =[]\n",
    "distance_between = {}\n",
    "\n",
    "for row in combinations(pairs, 2):\n",
    "    all_pairs.append(row)\n",
    "    \n",
    "for row in all_pairs:    \n",
    "    c1,c2 = row\n",
    "    distance_between[(c1,c2)] = geopy.distance.distance(c1, c2).km\n",
    "\n",
    "min(distance_between, key=distance_between.get)\n",
    "max(distance_between, key=distance_between.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7dfe3d9817a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# move on and loop through next row in DF capitals.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mcapitals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CapitalName'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'CapitalLatitude'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'CapitalLongitude'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcapitals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Take a random set of 10 capitals and get the weather there\n",
    "# Pick 10 capitals from DF df.\n",
    "# Get the weather.\n",
    "\n",
    "# For each row in the DF capitals, grab the lat and long and insert into the\n",
    "# request url above.\n",
    "# get the response and print it out.\n",
    "# move on and loop through next row in DF capitals.\n",
    "\n",
    "capitals = df[['CapitalName','CapitalLatitude','CapitalLongitude']].sample(10)\n",
    "\n",
    "for index, row in capitals.iterrows():\n",
    "    lat = row['CapitalLatitude']\n",
    "    long = row['CapitalLongitude']\n",
    "    url = 'https://api.openweathermap.org/data/2.5/weather?lat={}&lon={}&units=imperial&appid=d98b1e98a06b7f8ca0224b37bbd0df46'.format(lat, long)\n",
    "    weather = requests.get(url).json()\n",
    "    # Why doesn't this work? \n",
    "    # index['new_col'] = 'this is a value!'\n",
    "    capitals.at[index, 'response'] = weather['weather']\n",
    "    capitals.at[index, 'main'] = weather['weather'][0]['description']\n",
    "    capitals.at[index, 'description'] = weather['weather'][0]['description']\n",
    "    capitals.at[index, 'temp'] = weather['main']['temp']\n",
    "capitals\n",
    "\n",
    "# Another way.\n",
    "\n",
    "def get_weather_from_coords(row):\n",
    "    # row is just a row from a dataframe, I can access cols like row.colName or row['ColName']\n",
    "    key = '0bbf7572880155f0d500eca39a583571'\n",
    "    api = 'https://api.openweathermap.org/data/2.5/weather?lat={}&lon={}&appid={}&units=imperial'.format(row['CapitalLatitude'], row['CapitalLongitude'], key)\n",
    "    resp = requests.get(api).json() # I want a dictionary\n",
    "    return resp\n",
    "\n",
    "df = pd.read_json('http://techslides.com/demos/country-capitals.json')\n",
    "df = df.sample(n=5)\n",
    "df['weather_result'] = df.apply(get_weather_from_coords, axis=1)\n",
    "df.head()\n",
    "df['temp'] = df['weather_result'].map(lambda x: x.get('main').get('temp'))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Week10_AdvSQL/Day3/Activities/03-Stu_Dates/Solved/Stu_Dates.ipynb\n",
    "## http://localhost:8888/notebooks/UDEN201811DATA3/Week10_AdvSQL/Day3/Activities/03-Stu_Dates/Unsolved/Stu_Dates.ipynb\n",
    "## http://localhost:8888/notebooks/UDEN201811DATA3/Week10_AdvSQL/Day3/Activities/02-Ins_Dates/Solved/Ins_Dates.ipynb\n",
    "        \n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import geopy.distance\n",
    "\n",
    "# https://developers.google.com/places/web-service/search#PlaceSearchRequests\n",
    "\n",
    "target_coordinates = \"43.6187102, -116.2146068\"\n",
    "\n",
    "# set up a parameters dictionary\n",
    "params = {\n",
    "    \"location\": target_coordinates,\n",
    "    \"keyword\": \"starbucks\",\n",
    "    \"radius\": 50000,\n",
    "    \"key\": \"AIzaSyBTok6RlALK1Vm81gekooL9qxbi9zxOJSY\"\n",
    "}\n",
    "\n",
    "# base url\n",
    "base_url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
    "\n",
    "# run a request using our params dictionary\n",
    "response = requests.get(base_url, params=params).json()\n",
    "df = pd.DataFrame.from_records(response[\"results\"])\n",
    "df.head()\n",
    "\n",
    "# 1) how many starbucks did the API return?\n",
    "df[\"name\"].count()# 2) are they all starbucks, or are reg coffee shops in there?\n",
    "# Yep! All SBUX.\n",
    "df[\"name\"].value_counts()\n",
    "\n",
    "# 3) what is the average distance of all the Starbucks, from the target_coordinates?\n",
    "\n",
    "df[\"lat\"] = df['geometry'][0]['location']['lat']\n",
    "df[\"long\"] = df['geometry'][0]['location']['lng']\n",
    "\n",
    "#expected string: df[\"lat_long\"] = df[['lat', 'long']].apply(lambda x: ''.join(x), axis=1)\n",
    "\n",
    "def create_tuple(df_object):\n",
    "    return (df_object.lat, df_object.long)\n",
    "\n",
    "df['lat_long'] = df.apply(create_tuple, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Alchemy -- http://localhost:8888/notebooks/UDEN201811DATA3/Week10_AdvSQL/Day1/Activities/05-Ins_Preview_SQL_Alchemy/Solved/Alchemy.ipynb\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import pandas as pd\n",
    "# PyMySQL \n",
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "\n",
    "# Create Engine and Pass in MySQL Connection\n",
    "# engine = create_engine(\"mysql://k5xunpkmojyzse51:ifagg1gp7e2xyapi@ffn96u87j5ogvehy.cbetxkdyhwsb.us-east-1.rds.amazonaws.com:3306/tq6h098h0ym00zp6\")\n",
    "engine = create_engine('mysql://root:@127.0.0.1:3306/sakila') \n",
    "\n",
    "def get_custs(dbname, minid, maxid, table_name='customer'):\n",
    "    engine = create_engine('mysql://root:@127.0.0.1:3306/{}'.format(dbname))\n",
    "    customers = []\n",
    "    results = engine.execute(\"\"\"\n",
    "        SELECT * FROM {} c\n",
    "        WHERE c.customer_id BETWEEN {} and {};\n",
    "    \"\"\".format(table_name, minid, maxid))\n",
    "    return results\n",
    "\n",
    "res = get_custs('sakila',50,100,'customer')\n",
    "rows = res.fetchall() # list of tuples\n",
    "cols = res.keys() # column names\n",
    "df = pd.DataFrame.from_records(rows) #from_records also works with dictionaries\n",
    "df.columns = cols\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model a dice \n",
    "# N sides? - property\n",
    "# Behaviors - rolling, each time you roll it, you will see what side it lands on\n",
    "# random library?\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "class NSidedDie:\n",
    "    def __init__(self, nsides):\n",
    "        self.nsides = nsides\n",
    "        self.sides = list(range(1, self.nsides + 1))\n",
    "        self.created_at = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "    def roll(self):\n",
    "        return random.choice(self.sides)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"This is a dice with {} sides\".format(self.nsides)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"This is a dice with {} sides\".format(self.nsides)\n",
    "    \n",
    "    \n",
    "def let_them_play(p1, p2):\n",
    "    p1_roll, p2_roll = p1.roll(), p2.roll()\n",
    "    if p1_roll > p2_roll:\n",
    "        return \"Player1\"\n",
    "    elif p1_roll == p2_roll:\n",
    "        return \"Tie\"\n",
    "    else:\n",
    "        return \"Player2\"\n",
    "\n",
    "player1 = NSidedDie(1000)\n",
    "player2 = NSidedDie(6)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "results = []\n",
    "for x in range(100000):\n",
    "    results.append(let_them_play(player1, player2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Matplot lib\n",
    "import matplotlib\n",
    "from matplotlib import style\n",
    "style.use('seaborn')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import create_engine, inspect\n",
    "\n",
    "engine = create_engine(\"sqlite:///../Resources/emoji.sqlite\", echo=False)\n",
    "inspector = inspect(engine)\n",
    "\n",
    "inspector.get_table_names()\n",
    "\n",
    "inspector.get_columns('emoji')[0]\n",
    "\n",
    "result = engine.execute(\"SELECT * FROM emoji LIMIT 10\")\n",
    "for record in result:\n",
    "    print(record)\n",
    "    \n",
    "# Reflect Database into ORM class\n",
    "Base = automap_base()\n",
    "Base.prepare(engine, reflect=True)\n",
    "Emoji = Base.classes.emoji\n",
    "\n",
    "# Start a session to query the database\n",
    "session = Session(engine)\n",
    "\n",
    "# Query Emojis for `emoji_char`, `emoji_id`, and `score` and save the query into results\n",
    "\n",
    "data = session.query(Emoji)\n",
    "for record in data:\n",
    "  print(player.name_given)\n",
    "\n",
    "results = session.query(Emoji.emoji_char, Emoji.emoji_id, Emoji.score).\\\n",
    "    order_by(Emoji.score.desc()).all()\n",
    "\n",
    "# Unpack the `emoji_id` and `scores` from results and save into separate lists\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_df = []\n",
    "\n",
    "for row in session.query(Dow).filter(Dow.stock =='IBM').all():\n",
    "    for_df.append({\n",
    "        \"stock\": row.stock,\n",
    "        \"high\": row.high_price, \n",
    "        \"low\": row.low_price, \n",
    "        \"date\": row.date,\n",
    "        \"p2p\": row.high_price - row.low_price\n",
    "    })\n",
    "    \n",
    "out = pd.DataFrame.from_records(for_df)\n",
    "out.set_index('date', inplace=True)\n",
    "out.head("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function called `calc_temps` will accept start date and end date in the format '%Y-%m-%d' \n",
    "# and return the minimum, average, and maximum temperatures for that range of dates\n",
    "def calc_temps(start_date, end_date):\n",
    "    \"\"\"TMIN, TAVG, and TMAX for a list of dates.\n",
    "    \n",
    "    Args:\n",
    "        start_date (string): A date string in the format %Y-%m-%d\n",
    "        end_date (string): A date string in the format %Y-%m-%d\n",
    "        \n",
    "    Returns:\n",
    "        TMIN, TAVE, and TMAX\n",
    "    \"\"\"\n",
    "    \n",
    "    return session.query(func.min(Measurement.tobs), func.avg(Measurement.tobs), func.max(Measurement.tobs)).\\\n",
    "        filter(Measurement.date >= start_date).filter(Measurement.date <= end_date).all()\n",
    "\n",
    "# function usage example\n",
    "print(calc_temps('2012-02-28', '2012-03-05'))\n",
    "\n",
    "\n",
    "# Create a query that will calculate the daily normals \n",
    "# (i.e. the averages for tmin, tmax, and tavg for all historic data matching a specific month and day)\n",
    "\n",
    "def daily_normals(date):\n",
    "    \"\"\"Daily Normals.\n",
    "    \n",
    "    Args:\n",
    "        date (str): A date string in the format '%m-%d'\n",
    "        \n",
    "    Returns:\n",
    "        A list of tuples containing the daily normals, tmin, tavg, and tmax\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    sel = [func.min(Measurement.tobs), func.avg(Measurement.tobs), func.max(Measurement.tobs)]\n",
    "    return session.query(*sel).filter(func.strftime(\"%m-%d\", Measurement.date) == date).all()\n",
    "    \n",
    "daily_normals(\"01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "\n",
    "resp = requests.get(\"https://denver.craigslist.org/search/ataresp\")\n",
    "soup = BeautifulSoup(resp.content, \"html.parser\")\n",
    "rows = soup.find_all(\"li\", {\"class\": \"result-row\"})\n",
    "\n",
    "def get_item_name_and_price(soup_object):\n",
    "    try:\n",
    "        price = soup_object.find('span', {\"class\": \"result-price\"}).get_text()\n",
    "    except AttributeError:\n",
    "        price = \"cantPriceThatone\"\n",
    "    name = soup_object.find('a', {\"class\": \"result-title hdrlnk\"}).get_text()\n",
    "    #return (name, price)\n",
    "    return {\"item_name\": name, \"item_price\": price}\n",
    "\n",
    "item_prices = []\n",
    "\n",
    "for r in rows:\n",
    "    item_prices.append(get_item_name_and_price(r))\n",
    "    \n",
    "with open(\"item_prices.pkl\", \"wb\") as pickle_out:\n",
    "    pickle.dump(item_prices, pickle_out)\n",
    "    \n",
    "# databases\n",
    "# instead of tables, collections\n",
    "# insteaad of rows, documents\n",
    "\n",
    "import pymongo\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient()\n",
    "\n",
    "db = client.craigslist \n",
    "# this is a database . does not exist yet\n",
    "\n",
    "for row in item_prices:\n",
    "    # this is a collection on the db, will exist after I insert data\n",
    "    db.listings.insert_one(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import requests\n",
    "\n",
    "list_of_df = []\n",
    "\n",
    "for x in range(50):\n",
    "    # Grab the data.\n",
    "    data = requests.get(\"https://api.onepeloton.com/ecomm/product/bike/reviews?page={}\".format(x))\n",
    "    # Output JSON\n",
    "    out_json = data.json() #request will parse response into JSON or error if cannot parse.\n",
    "    # Convert to dataframe\n",
    "    out_df = pd.DataFrame.from_records(out_json.get('data'))\n",
    "    list_of_df.append(out_df)\n",
    "    \n",
    "peloton_df = pd.concat(list_of_df)\n",
    "peloton = peloton_df.to_dict('records')\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient()\n",
    "db = client.peloton \n",
    "# this is a database . does not exist yet\n",
    "\n",
    "for row in peloton:\n",
    "    # this is a collection on the db, will exist after I insert data\n",
    "    db.listings.insert_one(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "df = pd.read_csv('class.csv')\n",
    "\n",
    "\n",
    "# Goal: each file has a score for many genres, they are in columns named after the genre\n",
    "# for each FILE_NAME, gather all genres which have a score > 0\n",
    "# you might create a dictionary {filename: [g1, g2, ...], filename: [g4, g5]}\n",
    "# if the filename has no genres with a positive score, skip it\n",
    "\n",
    "# ANSWER\n",
    "\n",
    "results = {}\n",
    "for index, row in df.iterrows():\n",
    "    results[row.FILE_NAME] = {k:v for k,v in \n",
    "                              df.iloc[index][df.columns[4:]].to_dict().items() if v > 0}\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
